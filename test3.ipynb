{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"IMG_4434.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        \n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # Visualize the results\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"Tracking\", annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81a667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement model (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for model\n"
     ]
    }
   ],
   "source": [
    "pip install model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891a33ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные ключи в чекпоинте: dict_keys(['epoch', 'actor_type', 'net_type', 'net'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrackerModel:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.0.body.conv1.weight\", \"backbone.0.body.bn1.weight\", \"backbone.0.body.bn1.bias\", \"backbone.0.body.bn1.running_mean\", \"backbone.0.body.bn1.running_var\", \"backbone.0.body.layer1.0.conv1.weight\", \"backbone.0.body.layer1.0.bn1.weight\", \"backbone.0.body.layer1.0.bn1.bias\", \"backbone.0.body.layer1.0.bn1.running_mean\", \"backbone.0.body.layer1.0.bn1.running_var\", \"backbone.0.body.layer1.0.conv2.weight\", \"backbone.0.body.layer1.0.bn2.weight\", \"backbone.0.body.layer1.0.bn2.bias\", \"backbone.0.body.layer1.0.bn2.running_mean\", \"backbone.0.body.layer1.0.bn2.running_var\", \"backbone.0.body.layer1.0.conv3.weight\", \"backbone.0.body.layer1.0.bn3.weight\", \"backbone.0.body.layer1.0.bn3.bias\", \"backbone.0.body.layer1.0.bn3.running_mean\", \"backbone.0.body.layer1.0.bn3.running_var\", \"backbone.0.body.layer1.0.downsample.0.weight\", \"backbone.0.body.layer1.0.downsample.1.weight\", \"backbone.0.body.layer1.0.downsample.1.bias\", \"backbone.0.body.layer1.0.downsample.1.running_mean\", \"backbone.0.body.layer1.0.downsample.1.running_var\", \"backbone.0.body.layer1.1.conv1.weight\", \"backbone.0.body.layer1.1.bn1.weight\", \"backbone.0.body.layer1.1.bn1.bias\", \"backbone.0.body.layer1.1.bn1.running_mean\", \"backbone.0.body.layer1.1.bn1.running_var\", \"backbone.0.body.layer1.1.conv2.weight\", \"backbone.0.body.layer1.1.bn2.weight\", \"backbone.0.body.layer1.1.bn2.bias\", \"backbone.0.body.layer1.1.bn2.running_mean\", \"backbone.0.body.layer1.1.bn2.running_var\", \"backbone.0.body.layer1.1.conv3.weight\", \"backbone.0.body.layer1.1.bn3.weight\", \"backbone.0.body.layer1.1.bn3.bias\", \"backbone.0.body.layer1.1.bn3.running_mean\", \"backbone.0.body.layer1.1.bn3.running_var\", \"backbone.0.body.layer1.2.conv1.weight\", \"backbone.0.body.layer1.2.bn1.weight\", \"backbone.0.body.layer1.2.bn1.bias\", \"backbone.0.body.layer1.2.bn1.running_mean\", \"backbone.0.body.layer1.2.bn1.running_var\", \"backbone.0.body.layer1.2.conv2.weight\", \"backbone.0.body.layer1.2.bn2.weight\", \"backbone.0.body.layer1.2.bn2.bias\", \"backbone.0.body.layer1.2.bn2.running_mean\", \"backbone.0.body.layer1.2.bn2.running_var\", \"backbone.0.body.layer1.2.conv3.weight\", \"backbone.0.body.layer1.2.bn3.weight\", \"backbone.0.body.layer1.2.bn3.bias\", \"backbone.0.body.layer1.2.bn3.running_mean\", \"backbone.0.body.layer1.2.bn3.running_var\", \"backbone.0.body.layer2.0.conv1.weight\", \"backbone.0.body.layer2.0.bn1.weight\", \"backbone.0.body.layer2.0.bn1.bias\", \"backbone.0.body.layer2.0.bn1.running_mean\", \"backbone.0.body.layer2.0.bn1.running_var\", \"backbone.0.body.layer2.0.conv2.weight\", \"backbone.0.body.layer2.0.bn2.weight\", \"backbone.0.body.layer2.0.bn2.bias\", \"backbone.0.body.layer2.0.bn2.running_mean\", \"backbone.0.body.layer2.0.bn2.running_var\", \"backbone.0.body.layer2.0.conv3.weight\", \"backbone.0.body.layer2.0.bn3.weight\", \"backbone.0.body.layer2.0.bn3.bias\", \"backbone.0.body.layer2.0.bn3.running_mean\", \"backbone.0.body.layer2.0.bn3.running_var\", \"backbone.0.body.layer2.0.downsample.0.weight\", \"backbone.0.body.layer2.0.downsample.1.weight\", \"backbone.0.body.layer2.0.downsample.1.bias\", \"backbone.0.body.layer2.0.downsample.1.running_mean\", \"backbone.0.body.layer2.0.downsample.1.running_var\", \"backbone.0.body.layer2.1.conv1.weight\", \"backbone.0.body.layer2.1.bn1.weight\", \"backbone.0.body.layer2.1.bn1.bias\", \"backbone.0.body.layer2.1.bn1.running_mean\", \"backbone.0.body.layer2.1.bn1.running_var\", \"backbone.0.body.layer2.1.conv2.weight\", \"backbone.0.body.layer2.1.bn2.weight\", \"backbone.0.body.layer2.1.bn2.bias\", \"backbone.0.body.layer2.1.bn2.running_mean\", \"backbone.0.body.layer2.1.bn2.running_var\", \"backbone.0.body.layer2.1.conv3.weight\", \"backbone.0.body.layer2.1.bn3.weight\", \"backbone.0.body.layer2.1.bn3.bias\", \"backbone.0.body.layer2.1.bn3.running_mean\", \"backbone.0.body.layer2.1.bn3.running_var\", \"backbone.0.body.layer2.2.conv1.weight\", \"backbone.0.body.layer2.2.bn1.weight\", \"backbone.0.body.layer2.2.bn1.bias\", \"backbone.0.body.layer2.2.bn1.running_mean\", \"backbone.0.body.layer2.2.bn1.running_var\", \"backbone.0.body.layer2.2.conv2.weight\", \"backbone.0.body.layer2.2.bn2.weight\", \"backbone.0.body.layer2.2.bn2.bias\", \"backbone.0.body.layer2.2.bn2.running_mean\", \"backbone.0.body.layer2.2.bn2.running_var\", \"backbone.0.body.layer2.2.conv3.weight\", \"backbone.0.body.layer2.2.bn3.weight\", \"backbone.0.body.layer2.2.bn3.bias\", \"backbone.0.body.layer2.2.bn3.running_mean\", \"backbone.0.body.layer2.2.bn3.running_var\", \"backbone.0.body.layer2.3.conv1.weight\", \"backbone.0.body.layer2.3.bn1.weight\", \"backbone.0.body.layer2.3.bn1.bias\", \"backbone.0.body.layer2.3.bn1.running_mean\", \"backbone.0.body.layer2.3.bn1.running_var\", \"backbone.0.body.layer2.3.conv2.weight\", \"backbone.0.body.layer2.3.bn2.weight\", \"backbone.0.body.layer2.3.bn2.bias\", \"backbone.0.body.layer2.3.bn2.running_mean\", \"backbone.0.body.layer2.3.bn2.running_var\", \"backbone.0.body.layer2.3.conv3.weight\", \"backbone.0.body.layer2.3.bn3.weight\", \"backbone.0.body.layer2.3.bn3.bias\", \"backbone.0.body.layer2.3.bn3.running_mean\", \"backbone.0.body.layer2.3.bn3.running_var\", \"backbone.0.body.layer3.0.conv1.weight\", \"backbone.0.body.layer3.0.bn1.weight\", \"backbone.0.body.layer3.0.bn1.bias\", \"backbone.0.body.layer3.0.bn1.running_mean\", \"backbone.0.body.layer3.0.bn1.running_var\", \"backbone.0.body.layer3.0.conv2.weight\", \"backbone.0.body.layer3.0.bn2.weight\", \"backbone.0.body.layer3.0.bn2.bias\", \"backbone.0.body.layer3.0.bn2.running_mean\", \"backbone.0.body.layer3.0.bn2.running_var\", \"backbone.0.body.layer3.0.conv3.weight\", \"backbone.0.body.layer3.0.bn3.weight\", \"backbone.0.body.layer3.0.bn3.bias\", \"backbone.0.body.layer3.0.bn3.running_mean\", \"backbone.0.body.layer3.0.bn3.running_var\", \"backbone.0.body.layer3.0.downsample.0.weight\", \"backbone.0.body.layer3.0.downsample.1.weight\", \"backbone.0.body.layer3.0.downsample.1.bias\", \"backbone.0.body.layer3.0.downsample.1.running_mean\", \"backbone.0.body.layer3.0.downsample.1.running_var\", \"backbone.0.body.layer3.1.conv1.weight\", \"backbone.0.body.layer3.1.bn1.weight\", \"backbone.0.body.layer3.1.bn1.bias\", \"backbone.0.body.layer3.1.bn1.running_mean\", \"backbone.0.body.layer3.1.bn1.running_var\", \"backbone.0.body.layer3.1.conv2.weight\", \"backbone.0.body.layer3.1.bn2.weight\", \"backbone.0.body.layer3.1.bn2.bias\", \"backbone.0.body.layer3.1.bn2.running_mean\", \"backbone.0.body.layer3.1.bn2.running_var\", \"backbone.0.body.layer3.1.conv3.weight\", \"backbone.0.body.layer3.1.bn3.weight\", \"backbone.0.body.layer3.1.bn3.bias\", \"backbone.0.body.layer3.1.bn3.running_mean\", \"backbone.0.body.layer3.1.bn3.running_var\", \"backbone.0.body.layer3.2.conv1.weight\", \"backbone.0.body.layer3.2.bn1.weight\", \"backbone.0.body.layer3.2.bn1.bias\", \"backbone.0.body.layer3.2.bn1.running_mean\", \"backbone.0.body.layer3.2.bn1.running_var\", \"backbone.0.body.layer3.2.conv2.weight\", \"backbone.0.body.layer3.2.bn2.weight\", \"backbone.0.body.layer3.2.bn2.bias\", \"backbone.0.body.layer3.2.bn2.running_mean\", \"backbone.0.body.layer3.2.bn2.running_var\", \"backbone.0.body.layer3.2.conv3.weight\", \"backbone.0.body.layer3.2.bn3.weight\", \"backbone.0.body.layer3.2.bn3.bias\", \"backbone.0.body.layer3.2.bn3.running_mean\", \"backbone.0.body.layer3.2.bn3.running_var\", \"backbone.0.body.layer3.3.conv1.weight\", \"backbone.0.body.layer3.3.bn1.weight\", \"backbone.0.body.layer3.3.bn1.bias\", \"backbone.0.body.layer3.3.bn1.running_mean\", \"backbone.0.body.layer3.3.bn1.running_var\", \"backbone.0.body.layer3.3.conv2.weight\", \"backbone.0.body.layer3.3.bn2.weight\", \"backbone.0.body.layer3.3.bn2.bias\", \"backbone.0.body.layer3.3.bn2.running_mean\", \"backbone.0.body.layer3.3.bn2.running_var\", \"backbone.0.body.layer3.3.conv3.weight\", \"backbone.0.body.layer3.3.bn3.weight\", \"backbone.0.body.layer3.3.bn3.bias\", \"backbone.0.body.layer3.3.bn3.running_mean\", \"backbone.0.body.layer3.3.bn3.running_var\", \"backbone.0.body.layer3.4.conv1.weight\", \"backbone.0.body.layer3.4.bn1.weight\", \"backbone.0.body.layer3.4.bn1.bias\", \"backbone.0.body.layer3.4.bn1.running_mean\", \"backbone.0.body.layer3.4.bn1.running_var\", \"backbone.0.body.layer3.4.conv2.weight\", \"backbone.0.body.layer3.4.bn2.weight\", \"backbone.0.body.layer3.4.bn2.bias\", \"backbone.0.body.layer3.4.bn2.running_mean\", \"backbone.0.body.layer3.4.bn2.running_var\", \"backbone.0.body.layer3.4.conv3.weight\", \"backbone.0.body.layer3.4.bn3.weight\", \"backbone.0.body.layer3.4.bn3.bias\", \"backbone.0.body.layer3.4.bn3.running_mean\", \"backbone.0.body.layer3.4.bn3.running_var\", \"backbone.0.body.layer3.5.conv1.weight\", \"backbone.0.body.layer3.5.bn1.weight\", \"backbone.0.body.layer3.5.bn1.bias\", \"backbone.0.body.layer3.5.bn1.running_mean\", \"backbone.0.body.layer3.5.bn1.running_var\", \"backbone.0.body.layer3.5.conv2.weight\", \"backbone.0.body.layer3.5.bn2.weight\", \"backbone.0.body.layer3.5.bn2.bias\", \"backbone.0.body.layer3.5.bn2.running_mean\", \"backbone.0.body.layer3.5.bn2.running_var\", \"backbone.0.body.layer3.5.conv3.weight\", \"backbone.0.body.layer3.5.bn3.weight\", \"backbone.0.body.layer3.5.bn3.bias\", \"backbone.0.body.layer3.5.bn3.running_mean\", \"backbone.0.body.layer3.5.bn3.running_var\", \"transformer.encoder.layers.0.self_attn.in_proj_weight\", \"transformer.encoder.layers.0.self_attn.in_proj_bias\", \"transformer.encoder.layers.0.self_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.out_proj.bias\", \"transformer.encoder.layers.0.linear1.weight\", \"transformer.encoder.layers.0.linear1.bias\", \"transformer.encoder.layers.0.linear2.weight\", \"transformer.encoder.layers.0.linear2.bias\", \"transformer.encoder.layers.0.norm1.weight\", \"transformer.encoder.layers.0.norm1.bias\", \"transformer.encoder.layers.0.norm2.weight\", \"transformer.encoder.layers.0.norm2.bias\", \"transformer.encoder.layers.1.self_attn.in_proj_weight\", \"transformer.encoder.layers.1.self_attn.in_proj_bias\", \"transformer.encoder.layers.1.self_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.out_proj.bias\", \"transformer.encoder.layers.1.linear1.weight\", \"transformer.encoder.layers.1.linear1.bias\", \"transformer.encoder.layers.1.linear2.weight\", \"transformer.encoder.layers.1.linear2.bias\", \"transformer.encoder.layers.1.norm1.weight\", \"transformer.encoder.layers.1.norm1.bias\", \"transformer.encoder.layers.1.norm2.weight\", \"transformer.encoder.layers.1.norm2.bias\", \"transformer.encoder.layers.2.self_attn.in_proj_weight\", \"transformer.encoder.layers.2.self_attn.in_proj_bias\", \"transformer.encoder.layers.2.self_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.out_proj.bias\", \"transformer.encoder.layers.2.linear1.weight\", \"transformer.encoder.layers.2.linear1.bias\", \"transformer.encoder.layers.2.linear2.weight\", \"transformer.encoder.layers.2.linear2.bias\", \"transformer.encoder.layers.2.norm1.weight\", \"transformer.encoder.layers.2.norm1.bias\", \"transformer.encoder.layers.2.norm2.weight\", \"transformer.encoder.layers.2.norm2.bias\", \"transformer.decoder.layers.0.linear1.weight\", \"transformer.decoder.layers.0.linear1.bias\", \"transformer.decoder.layers.0.linear2.weight\", \"transformer.decoder.layers.0.linear2.bias\", \"transformer.decoder.layers.0.norm1.weight\", \"transformer.decoder.layers.0.norm1.bias\", \"transformer.decoder.layers.0.norm2.weight\", \"transformer.decoder.layers.0.norm2.bias\", \"transformer.decoder.norm.weight\", \"transformer.decoder.norm.bias\", \"box_head.conv1_tl.0.weight\", \"box_head.conv1_tl.0.bias\", \"box_head.conv1_tl.1.weight\", \"box_head.conv1_tl.1.bias\", \"box_head.conv1_tl.1.running_mean\", \"box_head.conv1_tl.1.running_var\", \"box_head.conv1_tl.1.num_batches_tracked\", \"box_head.conv2_tl.0.weight\", \"box_head.conv2_tl.0.bias\", \"box_head.conv2_tl.1.weight\", \"box_head.conv2_tl.1.bias\", \"box_head.conv2_tl.1.running_mean\", \"box_head.conv2_tl.1.running_var\", \"box_head.conv2_tl.1.num_batches_tracked\", \"box_head.conv3_tl.0.weight\", \"box_head.conv3_tl.0.bias\", \"box_head.conv3_tl.1.weight\", \"box_head.conv3_tl.1.bias\", \"box_head.conv3_tl.1.running_mean\", \"box_head.conv3_tl.1.running_var\", \"box_head.conv3_tl.1.num_batches_tracked\", \"box_head.conv4_tl.0.weight\", \"box_head.conv4_tl.0.bias\", \"box_head.conv4_tl.1.weight\", \"box_head.conv4_tl.1.bias\", \"box_head.conv4_tl.1.running_mean\", \"box_head.conv4_tl.1.running_var\", \"box_head.conv4_tl.1.num_batches_tracked\", \"box_head.conv5_tl.weight\", \"box_head.conv5_tl.bias\", \"box_head.conv1_br.0.weight\", \"box_head.conv1_br.0.bias\", \"box_head.conv1_br.1.weight\", \"box_head.conv1_br.1.bias\", \"box_head.conv1_br.1.running_mean\", \"box_head.conv1_br.1.running_var\", \"box_head.conv1_br.1.num_batches_tracked\", \"box_head.conv2_br.0.weight\", \"box_head.conv2_br.0.bias\", \"box_head.conv2_br.1.weight\", \"box_head.conv2_br.1.bias\", \"box_head.conv2_br.1.running_mean\", \"box_head.conv2_br.1.running_var\", \"box_head.conv2_br.1.num_batches_tracked\", \"box_head.conv3_br.0.weight\", \"box_head.conv3_br.0.bias\", \"box_head.conv3_br.1.weight\", \"box_head.conv3_br.1.bias\", \"box_head.conv3_br.1.running_mean\", \"box_head.conv3_br.1.running_var\", \"box_head.conv3_br.1.num_batches_tracked\", \"box_head.conv4_br.0.weight\", \"box_head.conv4_br.0.bias\", \"box_head.conv4_br.1.weight\", \"box_head.conv4_br.1.bias\", \"box_head.conv4_br.1.running_mean\", \"box_head.conv4_br.1.running_var\", \"box_head.conv4_br.1.num_batches_tracked\", \"box_head.conv5_br.weight\", \"box_head.conv5_br.bias\", \"foreground_embed.weight\", \"background_embed.weight\", \"bottleneck.weight\", \"bottleneck.bias\", \"iou_head.conv1.0.weight\", \"iou_head.conv1.0.bias\", \"iou_head.conv1.1.weight\", \"iou_head.conv1.1.bias\", \"iou_head.conv1.1.running_mean\", \"iou_head.conv1.1.running_var\", \"iou_head.conv1.1.num_batches_tracked\", \"iou_head.conv2.0.weight\", \"iou_head.conv2.0.bias\", \"iou_head.conv2.1.weight\", \"iou_head.conv2.1.bias\", \"iou_head.conv2.1.running_mean\", \"iou_head.conv2.1.running_var\", \"iou_head.conv2.1.num_batches_tracked\", \"iou_head.conv3.0.weight\", \"iou_head.conv3.0.bias\", \"iou_head.conv3.1.weight\", \"iou_head.conv3.1.bias\", \"iou_head.conv3.1.running_mean\", \"iou_head.conv3.1.running_var\", \"iou_head.conv3.1.num_batches_tracked\", \"iou_head.fc.linear.weight\", \"iou_head.fc.linear.bias\", \"iou_head.fc.bn.weight\", \"iou_head.fc.bn.bias\", \"iou_head.fc.bn.running_mean\", \"iou_head.fc.bn.running_var\", \"iou_head.fc.bn.num_batches_tracked\", \"iou_head.iou_predictor.weight\", \"iou_head.iou_predictor.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm2.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm2.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.long_term_attn.in_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.in_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_proj.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm1.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm1.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm2.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.long_term_attn.out_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.out_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.in_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.in_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm1.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm1.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm2.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.short_term_attn.out_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.out_proj.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Использование\u001b[39;00m\n\u001b[32m     56\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m model = \u001b[43mload_tracker_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAIATRACK_ep0500.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mload_tracker_model\u001b[39m\u001b[34m(model_path, device)\u001b[39m\n\u001b[32m     45\u001b[39m     new_state_dict[k] = v\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 6. Загрузка весов\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m model = model.to(device)\n\u001b[32m     50\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for TrackerModel:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.0.body.conv1.weight\", \"backbone.0.body.bn1.weight\", \"backbone.0.body.bn1.bias\", \"backbone.0.body.bn1.running_mean\", \"backbone.0.body.bn1.running_var\", \"backbone.0.body.layer1.0.conv1.weight\", \"backbone.0.body.layer1.0.bn1.weight\", \"backbone.0.body.layer1.0.bn1.bias\", \"backbone.0.body.layer1.0.bn1.running_mean\", \"backbone.0.body.layer1.0.bn1.running_var\", \"backbone.0.body.layer1.0.conv2.weight\", \"backbone.0.body.layer1.0.bn2.weight\", \"backbone.0.body.layer1.0.bn2.bias\", \"backbone.0.body.layer1.0.bn2.running_mean\", \"backbone.0.body.layer1.0.bn2.running_var\", \"backbone.0.body.layer1.0.conv3.weight\", \"backbone.0.body.layer1.0.bn3.weight\", \"backbone.0.body.layer1.0.bn3.bias\", \"backbone.0.body.layer1.0.bn3.running_mean\", \"backbone.0.body.layer1.0.bn3.running_var\", \"backbone.0.body.layer1.0.downsample.0.weight\", \"backbone.0.body.layer1.0.downsample.1.weight\", \"backbone.0.body.layer1.0.downsample.1.bias\", \"backbone.0.body.layer1.0.downsample.1.running_mean\", \"backbone.0.body.layer1.0.downsample.1.running_var\", \"backbone.0.body.layer1.1.conv1.weight\", \"backbone.0.body.layer1.1.bn1.weight\", \"backbone.0.body.layer1.1.bn1.bias\", \"backbone.0.body.layer1.1.bn1.running_mean\", \"backbone.0.body.layer1.1.bn1.running_var\", \"backbone.0.body.layer1.1.conv2.weight\", \"backbone.0.body.layer1.1.bn2.weight\", \"backbone.0.body.layer1.1.bn2.bias\", \"backbone.0.body.layer1.1.bn2.running_mean\", \"backbone.0.body.layer1.1.bn2.running_var\", \"backbone.0.body.layer1.1.conv3.weight\", \"backbone.0.body.layer1.1.bn3.weight\", \"backbone.0.body.layer1.1.bn3.bias\", \"backbone.0.body.layer1.1.bn3.running_mean\", \"backbone.0.body.layer1.1.bn3.running_var\", \"backbone.0.body.layer1.2.conv1.weight\", \"backbone.0.body.layer1.2.bn1.weight\", \"backbone.0.body.layer1.2.bn1.bias\", \"backbone.0.body.layer1.2.bn1.running_mean\", \"backbone.0.body.layer1.2.bn1.running_var\", \"backbone.0.body.layer1.2.conv2.weight\", \"backbone.0.body.layer1.2.bn2.weight\", \"backbone.0.body.layer1.2.bn2.bias\", \"backbone.0.body.layer1.2.bn2.running_mean\", \"backbone.0.body.layer1.2.bn2.running_var\", \"backbone.0.body.layer1.2.conv3.weight\", \"backbone.0.body.layer1.2.bn3.weight\", \"backbone.0.body.layer1.2.bn3.bias\", \"backbone.0.body.layer1.2.bn3.running_mean\", \"backbone.0.body.layer1.2.bn3.running_var\", \"backbone.0.body.layer2.0.conv1.weight\", \"backbone.0.body.layer2.0.bn1.weight\", \"backbone.0.body.layer2.0.bn1.bias\", \"backbone.0.body.layer2.0.bn1.running_mean\", \"backbone.0.body.layer2.0.bn1.running_var\", \"backbone.0.body.layer2.0.conv2.weight\", \"backbone.0.body.layer2.0.bn2.weight\", \"backbone.0.body.layer2.0.bn2.bias\", \"backbone.0.body.layer2.0.bn2.running_mean\", \"backbone.0.body.layer2.0.bn2.running_var\", \"backbone.0.body.layer2.0.conv3.weight\", \"backbone.0.body.layer2.0.bn3.weight\", \"backbone.0.body.layer2.0.bn3.bias\", \"backbone.0.body.layer2.0.bn3.running_mean\", \"backbone.0.body.layer2.0.bn3.running_var\", \"backbone.0.body.layer2.0.downsample.0.weight\", \"backbone.0.body.layer2.0.downsample.1.weight\", \"backbone.0.body.layer2.0.downsample.1.bias\", \"backbone.0.body.layer2.0.downsample.1.running_mean\", \"backbone.0.body.layer2.0.downsample.1.running_var\", \"backbone.0.body.layer2.1.conv1.weight\", \"backbone.0.body.layer2.1.bn1.weight\", \"backbone.0.body.layer2.1.bn1.bias\", \"backbone.0.body.layer2.1.bn1.running_mean\", \"backbone.0.body.layer2.1.bn1.running_var\", \"backbone.0.body.layer2.1.conv2.weight\", \"backbone.0.body.layer2.1.bn2.weight\", \"backbone.0.body.layer2.1.bn2.bias\", \"backbone.0.body.layer2.1.bn2.running_mean\", \"backbone.0.body.layer2.1.bn2.running_var\", \"backbone.0.body.layer2.1.conv3.weight\", \"backbone.0.body.layer2.1.bn3.weight\", \"backbone.0.body.layer2.1.bn3.bias\", \"backbone.0.body.layer2.1.bn3.running_mean\", \"backbone.0.body.layer2.1.bn3.running_var\", \"backbone.0.body.layer2.2.conv1.weight\", \"backbone.0.body.layer2.2.bn1.weight\", \"backbone.0.body.layer2.2.bn1.bias\", \"backbone.0.body.layer2.2.bn1.running_mean\", \"backbone.0.body.layer2.2.bn1.running_var\", \"backbone.0.body.layer2.2.conv2.weight\", \"backbone.0.body.layer2.2.bn2.weight\", \"backbone.0.body.layer2.2.bn2.bias\", \"backbone.0.body.layer2.2.bn2.running_mean\", \"backbone.0.body.layer2.2.bn2.running_var\", \"backbone.0.body.layer2.2.conv3.weight\", \"backbone.0.body.layer2.2.bn3.weight\", \"backbone.0.body.layer2.2.bn3.bias\", \"backbone.0.body.layer2.2.bn3.running_mean\", \"backbone.0.body.layer2.2.bn3.running_var\", \"backbone.0.body.layer2.3.conv1.weight\", \"backbone.0.body.layer2.3.bn1.weight\", \"backbone.0.body.layer2.3.bn1.bias\", \"backbone.0.body.layer2.3.bn1.running_mean\", \"backbone.0.body.layer2.3.bn1.running_var\", \"backbone.0.body.layer2.3.conv2.weight\", \"backbone.0.body.layer2.3.bn2.weight\", \"backbone.0.body.layer2.3.bn2.bias\", \"backbone.0.body.layer2.3.bn2.running_mean\", \"backbone.0.body.layer2.3.bn2.running_var\", \"backbone.0.body.layer2.3.conv3.weight\", \"backbone.0.body.layer2.3.bn3.weight\", \"backbone.0.body.layer2.3.bn3.bias\", \"backbone.0.body.layer2.3.bn3.running_mean\", \"backbone.0.body.layer2.3.bn3.running_var\", \"backbone.0.body.layer3.0.conv1.weight\", \"backbone.0.body.layer3.0.bn1.weight\", \"backbone.0.body.layer3.0.bn1.bias\", \"backbone.0.body.layer3.0.bn1.running_mean\", \"backbone.0.body.layer3.0.bn1.running_var\", \"backbone.0.body.layer3.0.conv2.weight\", \"backbone.0.body.layer3.0.bn2.weight\", \"backbone.0.body.layer3.0.bn2.bias\", \"backbone.0.body.layer3.0.bn2.running_mean\", \"backbone.0.body.layer3.0.bn2.running_var\", \"backbone.0.body.layer3.0.conv3.weight\", \"backbone.0.body.layer3.0.bn3.weight\", \"backbone.0.body.layer3.0.bn3.bias\", \"backbone.0.body.layer3.0.bn3.running_mean\", \"backbone.0.body.layer3.0.bn3.running_var\", \"backbone.0.body.layer3.0.downsample.0.weight\", \"backbone.0.body.layer3.0.downsample.1.weight\", \"backbone.0.body.layer3.0.downsample.1.bias\", \"backbone.0.body.layer3.0.downsample.1.running_mean\", \"backbone.0.body.layer3.0.downsample.1.running_var\", \"backbone.0.body.layer3.1.conv1.weight\", \"backbone.0.body.layer3.1.bn1.weight\", \"backbone.0.body.layer3.1.bn1.bias\", \"backbone.0.body.layer3.1.bn1.running_mean\", \"backbone.0.body.layer3.1.bn1.running_var\", \"backbone.0.body.layer3.1.conv2.weight\", \"backbone.0.body.layer3.1.bn2.weight\", \"backbone.0.body.layer3.1.bn2.bias\", \"backbone.0.body.layer3.1.bn2.running_mean\", \"backbone.0.body.layer3.1.bn2.running_var\", \"backbone.0.body.layer3.1.conv3.weight\", \"backbone.0.body.layer3.1.bn3.weight\", \"backbone.0.body.layer3.1.bn3.bias\", \"backbone.0.body.layer3.1.bn3.running_mean\", \"backbone.0.body.layer3.1.bn3.running_var\", \"backbone.0.body.layer3.2.conv1.weight\", \"backbone.0.body.layer3.2.bn1.weight\", \"backbone.0.body.layer3.2.bn1.bias\", \"backbone.0.body.layer3.2.bn1.running_mean\", \"backbone.0.body.layer3.2.bn1.running_var\", \"backbone.0.body.layer3.2.conv2.weight\", \"backbone.0.body.layer3.2.bn2.weight\", \"backbone.0.body.layer3.2.bn2.bias\", \"backbone.0.body.layer3.2.bn2.running_mean\", \"backbone.0.body.layer3.2.bn2.running_var\", \"backbone.0.body.layer3.2.conv3.weight\", \"backbone.0.body.layer3.2.bn3.weight\", \"backbone.0.body.layer3.2.bn3.bias\", \"backbone.0.body.layer3.2.bn3.running_mean\", \"backbone.0.body.layer3.2.bn3.running_var\", \"backbone.0.body.layer3.3.conv1.weight\", \"backbone.0.body.layer3.3.bn1.weight\", \"backbone.0.body.layer3.3.bn1.bias\", \"backbone.0.body.layer3.3.bn1.running_mean\", \"backbone.0.body.layer3.3.bn1.running_var\", \"backbone.0.body.layer3.3.conv2.weight\", \"backbone.0.body.layer3.3.bn2.weight\", \"backbone.0.body.layer3.3.bn2.bias\", \"backbone.0.body.layer3.3.bn2.running_mean\", \"backbone.0.body.layer3.3.bn2.running_var\", \"backbone.0.body.layer3.3.conv3.weight\", \"backbone.0.body.layer3.3.bn3.weight\", \"backbone.0.body.layer3.3.bn3.bias\", \"backbone.0.body.layer3.3.bn3.running_mean\", \"backbone.0.body.layer3.3.bn3.running_var\", \"backbone.0.body.layer3.4.conv1.weight\", \"backbone.0.body.layer3.4.bn1.weight\", \"backbone.0.body.layer3.4.bn1.bias\", \"backbone.0.body.layer3.4.bn1.running_mean\", \"backbone.0.body.layer3.4.bn1.running_var\", \"backbone.0.body.layer3.4.conv2.weight\", \"backbone.0.body.layer3.4.bn2.weight\", \"backbone.0.body.layer3.4.bn2.bias\", \"backbone.0.body.layer3.4.bn2.running_mean\", \"backbone.0.body.layer3.4.bn2.running_var\", \"backbone.0.body.layer3.4.conv3.weight\", \"backbone.0.body.layer3.4.bn3.weight\", \"backbone.0.body.layer3.4.bn3.bias\", \"backbone.0.body.layer3.4.bn3.running_mean\", \"backbone.0.body.layer3.4.bn3.running_var\", \"backbone.0.body.layer3.5.conv1.weight\", \"backbone.0.body.layer3.5.bn1.weight\", \"backbone.0.body.layer3.5.bn1.bias\", \"backbone.0.body.layer3.5.bn1.running_mean\", \"backbone.0.body.layer3.5.bn1.running_var\", \"backbone.0.body.layer3.5.conv2.weight\", \"backbone.0.body.layer3.5.bn2.weight\", \"backbone.0.body.layer3.5.bn2.bias\", \"backbone.0.body.layer3.5.bn2.running_mean\", \"backbone.0.body.layer3.5.bn2.running_var\", \"backbone.0.body.layer3.5.conv3.weight\", \"backbone.0.body.layer3.5.bn3.weight\", \"backbone.0.body.layer3.5.bn3.bias\", \"backbone.0.body.layer3.5.bn3.running_mean\", \"backbone.0.body.layer3.5.bn3.running_var\", \"transformer.encoder.layers.0.self_attn.in_proj_weight\", \"transformer.encoder.layers.0.self_attn.in_proj_bias\", \"transformer.encoder.layers.0.self_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.out_proj.bias\", \"transformer.encoder.layers.0.linear1.weight\", \"transformer.encoder.layers.0.linear1.bias\", \"transformer.encoder.layers.0.linear2.weight\", \"transformer.encoder.layers.0.linear2.bias\", \"transformer.encoder.layers.0.norm1.weight\", \"transformer.encoder.layers.0.norm1.bias\", \"transformer.encoder.layers.0.norm2.weight\", \"transformer.encoder.layers.0.norm2.bias\", \"transformer.encoder.layers.1.self_attn.in_proj_weight\", \"transformer.encoder.layers.1.self_attn.in_proj_bias\", \"transformer.encoder.layers.1.self_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.out_proj.bias\", \"transformer.encoder.layers.1.linear1.weight\", \"transformer.encoder.layers.1.linear1.bias\", \"transformer.encoder.layers.1.linear2.weight\", \"transformer.encoder.layers.1.linear2.bias\", \"transformer.encoder.layers.1.norm1.weight\", \"transformer.encoder.layers.1.norm1.bias\", \"transformer.encoder.layers.1.norm2.weight\", \"transformer.encoder.layers.1.norm2.bias\", \"transformer.encoder.layers.2.self_attn.in_proj_weight\", \"transformer.encoder.layers.2.self_attn.in_proj_bias\", \"transformer.encoder.layers.2.self_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.out_proj.bias\", \"transformer.encoder.layers.2.linear1.weight\", \"transformer.encoder.layers.2.linear1.bias\", \"transformer.encoder.layers.2.linear2.weight\", \"transformer.encoder.layers.2.linear2.bias\", \"transformer.encoder.layers.2.norm1.weight\", \"transformer.encoder.layers.2.norm1.bias\", \"transformer.encoder.layers.2.norm2.weight\", \"transformer.encoder.layers.2.norm2.bias\", \"transformer.decoder.layers.0.linear1.weight\", \"transformer.decoder.layers.0.linear1.bias\", \"transformer.decoder.layers.0.linear2.weight\", \"transformer.decoder.layers.0.linear2.bias\", \"transformer.decoder.layers.0.norm1.weight\", \"transformer.decoder.layers.0.norm1.bias\", \"transformer.decoder.layers.0.norm2.weight\", \"transformer.decoder.layers.0.norm2.bias\", \"transformer.decoder.norm.weight\", \"transformer.decoder.norm.bias\", \"box_head.conv1_tl.0.weight\", \"box_head.conv1_tl.0.bias\", \"box_head.conv1_tl.1.weight\", \"box_head.conv1_tl.1.bias\", \"box_head.conv1_tl.1.running_mean\", \"box_head.conv1_tl.1.running_var\", \"box_head.conv1_tl.1.num_batches_tracked\", \"box_head.conv2_tl.0.weight\", \"box_head.conv2_tl.0.bias\", \"box_head.conv2_tl.1.weight\", \"box_head.conv2_tl.1.bias\", \"box_head.conv2_tl.1.running_mean\", \"box_head.conv2_tl.1.running_var\", \"box_head.conv2_tl.1.num_batches_tracked\", \"box_head.conv3_tl.0.weight\", \"box_head.conv3_tl.0.bias\", \"box_head.conv3_tl.1.weight\", \"box_head.conv3_tl.1.bias\", \"box_head.conv3_tl.1.running_mean\", \"box_head.conv3_tl.1.running_var\", \"box_head.conv3_tl.1.num_batches_tracked\", \"box_head.conv4_tl.0.weight\", \"box_head.conv4_tl.0.bias\", \"box_head.conv4_tl.1.weight\", \"box_head.conv4_tl.1.bias\", \"box_head.conv4_tl.1.running_mean\", \"box_head.conv4_tl.1.running_var\", \"box_head.conv4_tl.1.num_batches_tracked\", \"box_head.conv5_tl.weight\", \"box_head.conv5_tl.bias\", \"box_head.conv1_br.0.weight\", \"box_head.conv1_br.0.bias\", \"box_head.conv1_br.1.weight\", \"box_head.conv1_br.1.bias\", \"box_head.conv1_br.1.running_mean\", \"box_head.conv1_br.1.running_var\", \"box_head.conv1_br.1.num_batches_tracked\", \"box_head.conv2_br.0.weight\", \"box_head.conv2_br.0.bias\", \"box_head.conv2_br.1.weight\", \"box_head.conv2_br.1.bias\", \"box_head.conv2_br.1.running_mean\", \"box_head.conv2_br.1.running_var\", \"box_head.conv2_br.1.num_batches_tracked\", \"box_head.conv3_br.0.weight\", \"box_head.conv3_br.0.bias\", \"box_head.conv3_br.1.weight\", \"box_head.conv3_br.1.bias\", \"box_head.conv3_br.1.running_mean\", \"box_head.conv3_br.1.running_var\", \"box_head.conv3_br.1.num_batches_tracked\", \"box_head.conv4_br.0.weight\", \"box_head.conv4_br.0.bias\", \"box_head.conv4_br.1.weight\", \"box_head.conv4_br.1.bias\", \"box_head.conv4_br.1.running_mean\", \"box_head.conv4_br.1.running_var\", \"box_head.conv4_br.1.num_batches_tracked\", \"box_head.conv5_br.weight\", \"box_head.conv5_br.bias\", \"foreground_embed.weight\", \"background_embed.weight\", \"bottleneck.weight\", \"bottleneck.bias\", \"iou_head.conv1.0.weight\", \"iou_head.conv1.0.bias\", \"iou_head.conv1.1.weight\", \"iou_head.conv1.1.bias\", \"iou_head.conv1.1.running_mean\", \"iou_head.conv1.1.running_var\", \"iou_head.conv1.1.num_batches_tracked\", \"iou_head.conv2.0.weight\", \"iou_head.conv2.0.bias\", \"iou_head.conv2.1.weight\", \"iou_head.conv2.1.bias\", \"iou_head.conv2.1.running_mean\", \"iou_head.conv2.1.running_var\", \"iou_head.conv2.1.num_batches_tracked\", \"iou_head.conv3.0.weight\", \"iou_head.conv3.0.bias\", \"iou_head.conv3.1.weight\", \"iou_head.conv3.1.bias\", \"iou_head.conv3.1.running_mean\", \"iou_head.conv3.1.running_var\", \"iou_head.conv3.1.num_batches_tracked\", \"iou_head.fc.linear.weight\", \"iou_head.fc.linear.bias\", \"iou_head.fc.bn.weight\", \"iou_head.fc.bn.bias\", \"iou_head.fc.bn.running_mean\", \"iou_head.fc.bn.running_var\", \"iou_head.fc.bn.num_batches_tracked\", \"iou_head.iou_predictor.weight\", \"iou_head.iou_predictor.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.0.self_attn.inner_attn.feat_norm2.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.1.self_attn.inner_attn.feat_norm2.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_proj.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_proj.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm1.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm1.bias\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm2.weight\", \"transformer.encoder.layers.2.self_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.long_term_attn.in_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.in_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_proj.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm1.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm1.bias\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm2.weight\", \"transformer.decoder.layers.0.long_term_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.long_term_attn.out_proj.weight\", \"transformer.decoder.layers.0.long_term_attn.out_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.in_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.in_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.q_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.k_proj_weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.q_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.k_proj_bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.out_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.corr_attn.out_proj.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm1.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm1.bias\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm2.weight\", \"transformer.decoder.layers.0.short_term_attn.inner_attn.feat_norm2.bias\", \"transformer.decoder.layers.0.short_term_attn.out_proj.weight\", \"transformer.decoder.layers.0.short_term_attn.out_proj.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Пример класса модели\n",
    "class TrackerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
    "        self.fc = nn.Linear(64*10*10, 4)  # Пример для bounding box [x, y, w, h]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Загрузка модели\n",
    "def load_tracker_model(model_path, device='cuda'):\n",
    "    \"\"\"Загрузка модели из .pth.tar файла\"\"\"\n",
    "    # 1. Инициализация модели\n",
    "    model = TrackerModel()\n",
    "    \n",
    "    # 2. Загрузка чекпоинта\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # 3. Проверка доступных ключей\n",
    "    print(\"Доступные ключи в чекпоинте:\", checkpoint.keys())\n",
    "    \n",
    "    # 4. Определение ключа для весов\n",
    "    state_dict_key = None\n",
    "    for key in ['state_dict', 'model_state', 'model', 'net']:\n",
    "        if key in checkpoint:\n",
    "            state_dict_key = key\n",
    "            break\n",
    "    \n",
    "    if state_dict_key is None:\n",
    "        state_dict = checkpoint  # Если напрямую сохранен state_dict\n",
    "    else:\n",
    "        state_dict = checkpoint[state_dict_key]\n",
    "    \n",
    "    # 5. Обработка DataParallel (удаление префикса 'module.')\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:]  # Удаление 'module.'\n",
    "        new_state_dict[k] = v\n",
    "    \n",
    "    # 6. Загрузка весов\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Модель успешно загружена из {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Использование\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_tracker_model('AIATRACK_ep0500.pth.tar', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b9c89c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YourModelClass  \u001b[38;5;66;03m# Импортируйте класс вашей модели\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Инициализация модели\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import model\n",
    "from model import YourModelClass  # Импортируйте класс вашей модели\n",
    "\n",
    "# Инициализация модели\n",
    "model = YourModelClass()\n",
    "\n",
    "# Загрузка чекпоинта\n",
    "checkpoint = torch.load('PiVOT_L_27.pth.tar', map_location='cpu')  \n",
    "\n",
    "# Загрузка состояния модели\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# Переключение в режим оценки\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba5f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_338568\\779763916.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  model = YOLO('C:\\ИТМО\\НИР 2\\data\\AIATRACK_ep0500.pth.tar')  # Или .pt файла\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_338568\\779763916.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  model = YOLO('C:\\ИТМО\\НИР 2\\data\\AIATRACK_ep0500.pth.tar')  # Или .pt файла\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "C:\\ИТМО\\НИР 2\\data\\AIATRACK_ep0500.pth.tar acceptable suffix is {'.pt'}, not .tar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mИТМО\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mНИР 2\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mAIATRACK_ep0500.pth.tar\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Или .pt файла\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Пример использования для трекинга\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mIMG_4434.MP4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracker\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset/data.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Используйте любой доступный трекер\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\engine\\model.py:599\u001b[39m, in \u001b[36mModel.track\u001b[39m\u001b[34m(self, source, stream, persist, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# batch-size 1 for tracking in videos\u001b[39;00m\n\u001b[32m    598\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtrack\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\engine\\model.py:548\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor:\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor = (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mpredictor\u001b[39m\u001b[33m\"\u001b[39m))(overrides=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.args = get_cfg(\u001b[38;5;28mself\u001b[39m.predictor.args, args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:391\u001b[39m, in \u001b[36mBasePredictor.setup_model\u001b[39m\u001b[34m(self, model, verbose)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    Initialize YOLO model with given parameters and set it to evaluation mode.\u001b[39;00m\n\u001b[32m    386\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m \u001b[33;03m        verbose (bool): Whether to print verbose output.\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28mself\u001b[39m.model.device  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.half = \u001b[38;5;28mself\u001b[39m.model.fp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:215\u001b[39m, in \u001b[36mAutoBackend.__init__\u001b[39m\u001b[34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pt:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attempt_load_weights\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     model = \u001b[43mattempt_load_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuse\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33mkpt_shape\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    219\u001b[39m         kpt_shape = model.kpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1501\u001b[39m, in \u001b[36mattempt_load_weights\u001b[39m\u001b[34m(weights, device, inplace, fuse)\u001b[39m\n\u001b[32m   1499\u001b[39m ensemble = Ensemble()\n\u001b[32m   1500\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     ckpt, w = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1502\u001b[39m     args = {**DEFAULT_CFG_DICT, **ckpt[\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# combined args\u001b[39;00m\n\u001b[32m   1503\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1424\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1405\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1406\u001b[39m \u001b[33;03mAttempt to load a PyTorch model with the torch.load() function. If a ModuleNotFoundError is raised, it catches the\u001b[39;00m\n\u001b[32m   1407\u001b[39m \u001b[33;03merror, logs a warning message, and attempts to install the missing module via the check_requirements() function.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1420\u001b[39m \u001b[33;03m    >>> ckpt, file = torch_safe_load(\"path/to/best.pt\", safe_only=True)\u001b[39;00m\n\u001b[32m   1421\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloads\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m attempt_download_asset\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m \u001b[43mcheck_suffix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m file = attempt_download_asset(weight)  \u001b[38;5;66;03m# search online if missing locally\u001b[39;00m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\utils\\checks.py:487\u001b[39m, in \u001b[36mcheck_suffix\u001b[39m\u001b[34m(file, suffix, msg)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m file \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m [file]:\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m s := \u001b[38;5;28mstr\u001b[39m(f).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].lower().strip():  \u001b[38;5;66;03m# file suffix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m suffix, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acceptable suffix is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, not .\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: C:\\ИТМО\\НИР 2\\data\\AIATRACK_ep0500.pth.tar acceptable suffix is {'.pt'}, not .tar"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Загрузка модели из .pth.tar файла\n",
    "model = YOLO('C:\\ИТМО\\НИР 2\\data\\')  # Или .pt файла\n",
    "\n",
    "# Пример использования для трекинга\n",
    "results = model.track(\n",
    "    source='IMG_4434.MP4',\n",
    "    tracker='dataset/data.yaml',  # Используйте любой доступный трекер\n",
    "    persist=True,\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f37f8e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ltr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Просмотр содержимого без загрузки модели\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPiVOT_L_27.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mКлючи:\u001b[39m\u001b[33m\"\u001b[39m, checkpoint.keys())\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mАрхитектура:\u001b[39m\u001b[33m\"\u001b[39m, checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]._modules.keys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ultralytics\\utils\\patches.py:117\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    115\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:1525\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1523\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1524\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1525\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1533\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:2114\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   2113\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m2114\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2115\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2117\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:2103\u001b[39m, in \u001b[36m_load.<locals>.UnpicklerWrapper.find_class\u001b[39m\u001b[34m(self, mod_name, name)\u001b[39m\n\u001b[32m   2101\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2102\u001b[39m mod_name = load_module_mapping.get(mod_name, mod_name)\n\u001b[32m-> \u001b[39m\u001b[32m2103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ltr'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Просмотр содержимого без загрузки модели\n",
    "checkpoint = torch.load('PiVOT_L_27.pth.tar', map_location='cpu')\n",
    "print(\"Ключи:\", checkpoint.keys())\n",
    "print(\"Архитектура:\", checkpoint['model']._modules.keys() if 'model' in checkpoint else \"N/A\")\n",
    "print(\"Размеры весов первого слоя:\", list(checkpoint['state_dict'].values())[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f985bb40",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ostrack_model.pth.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Использование\u001b[39;00m\n\u001b[32m     56\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m model = \u001b[43mload_tracker_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mostrack_model.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mload_tracker_model\u001b[39m\u001b[34m(model_path, device)\u001b[39m\n\u001b[32m     20\u001b[39m model = TrackerModel()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2. Загрузка чекпоинта\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 3. Проверка доступных ключей\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mДоступные ключи в чекпоинте:\u001b[39m\u001b[33m\"\u001b[39m, checkpoint.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ostrack_model.pth.tar'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Пример класса модели\n",
    "class TrackerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n",
    "        self.fc = nn.Linear(64*10*10, 4)  # Пример для bounding box [x, y, w, h]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Загрузка модели\n",
    "def load_tracker_model(model_path, device='cuda'):\n",
    "    \"\"\"Загрузка модели из .pth.tar файла\"\"\"\n",
    "    # 1. Инициализация модели\n",
    "    model = TrackerModel()\n",
    "    \n",
    "    # 2. Загрузка чекпоинта\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # 3. Проверка доступных ключей\n",
    "    print(\"Доступные ключи в чекпоинте:\", checkpoint.keys())\n",
    "    \n",
    "    # 4. Определение ключа для весов\n",
    "    state_dict_key = None\n",
    "    for key in ['state_dict', 'model_state', 'model', 'net']:\n",
    "        if key in checkpoint:\n",
    "            state_dict_key = key\n",
    "            break\n",
    "    \n",
    "    if state_dict_key is None:\n",
    "        state_dict = checkpoint  # Если напрямую сохранен state_dict\n",
    "    else:\n",
    "        state_dict = checkpoint[state_dict_key]\n",
    "    \n",
    "    # 5. Обработка DataParallel (удаление префикса 'module.')\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:]  # Удаление 'module.'\n",
    "        new_state_dict[k] = v\n",
    "    \n",
    "    # 6. Загрузка весов\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Модель успешно загружена из {model_path}\")\n",
    "    return model\n",
    "\n",
    "# Использование\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_tracker_model('ostrack_model.pth.tar', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eed0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackerWrapper:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = load_tracker_model(model_path)\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.initialized = False\n",
    "    \n",
    "    def init(self, frame, bbox):\n",
    "        \"\"\"Инициализация трекера на первом кадре\"\"\"\n",
    "        # Преобразование кадра в тензор\n",
    "        self.template = self._preprocess(frame, bbox)\n",
    "        self.initialized = True\n",
    "        return True\n",
    "    \n",
    "    def update(self, frame):\n",
    "        \"\"\"Обновление трекера на новом кадре\"\"\"\n",
    "        if not self.initialized:\n",
    "            raise RuntimeError(\"Трекер не инициализирован\")\n",
    "        \n",
    "        # Преобразование кадра в тензор\n",
    "        search_region = self._preprocess_search(frame)\n",
    "        \n",
    "        # Инференс\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(search_region)\n",
    "            bbox = self._decode_prediction(prediction)\n",
    "        \n",
    "        return True, bbox\n",
    "    \n",
    "    def _preprocess(self, frame, bbox):\n",
    "        \"\"\"Предобработка шаблона\"\"\"\n",
    "        # 1. Вырезание региона по bbox\n",
    "        # 2. Нормализация\n",
    "        # 3. Преобразование в тензор\n",
    "        return tensor.to(self.device)\n",
    "    \n",
    "    def _preprocess_search(self, frame):\n",
    "        \"\"\"Предобработка области поиска\"\"\"\n",
    "        # На основе предыдущей позиции\n",
    "        return tensor.to(self.device)\n",
    "    \n",
    "    def _decode_prediction(self, output):\n",
    "        \"\"\"Преобразование выхода модели в bbox\"\"\"\n",
    "        # Пример: [x, y, w, h]\n",
    "        return output.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762ac27c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_tracker_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Инициализация\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tracker = \u001b[43mTrackerWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mostrack_model.pth.tar\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m video = cv2.VideoCapture(\u001b[33m'\u001b[39m\u001b[33mIMG_4434.MP4\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m ret, frame = video.read()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mTrackerWrapper.__init__\u001b[39m\u001b[34m(self, model_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mload_tracker_model\u001b[49m(model_path)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.parameters()).device\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialized = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_tracker_model' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Инициализация\n",
    "tracker = TrackerWrapper('ostrack_model.pth.tar')\n",
    "video = cv2.VideoCapture('IMG_4434.MP4')\n",
    "\n",
    "ret, frame = video.read()\n",
    "init_bbox = [x, y, w, h]  # Начальный bounding box\n",
    "tracker.init(frame, init_bbox)\n",
    "\n",
    "# Обработка видео\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Обновление трекера\n",
    "    success, bbox = tracker.update(frame)\n",
    "    \n",
    "    # Отрисовка результатов\n",
    "    if success:\n",
    "        x, y, w, h = [int(v) for v in bbox]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Tracking', frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4d724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mixformer(model_path):\n",
    "    from mixformer import build_mixformer\n",
    "    \n",
    "    model = build_mixformer()\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Обработка различных форматов\n",
    "    if 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    elif 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Удаление префиксов\n",
    "    state_dict = {k.replace('module.', '').replace('backbone.', ''): v \n",
    "                 for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b73aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Инициализация\n",
    "tracker = TrackerWrapper('ostrack_model.pth.tar')\n",
    "video = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "ret, frame = video.read()\n",
    "init_bbox = [x, y, w, h]  # Начальный bounding box\n",
    "tracker.init(frame, init_bbox)\n",
    "\n",
    "# Обработка видео\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Обновление трекера\n",
    "    success, bbox = tracker.update(frame)\n",
    "    \n",
    "    # Отрисовка результатов\n",
    "    if success:\n",
    "        x, y, w, h = [int(v) for v in bbox]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Tracking', frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09710c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
